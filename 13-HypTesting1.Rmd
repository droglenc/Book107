# Hypothesis Testing - Introduction {#HypTesting1}
A statistic is an imperfect estimate of a parameter because of sampling variability. There are two calculations using the results of a single sample that recognize this imperfection and allow conclusions to be made about a parameter. First, a researcher may form an *a priori* hypothesis about a parameter and then use the information in the sample to make a judgment about the "correctness" of that hypothesis. Second, a researcher may form, from the information in the sample, a range of values that is likely to contain the parameter. The first method is called *hypothesis testing* and is the subject of this module. The second method consists of constructing a *confidence region*, which is introduced in Module \@ref(ConfRegions1). Specific applications of these two techniques are described in Modules \@ref(1Ztest)-\@ref(GOFTest).


## Hypothesis Testing & The Scientific Method
In its simplest form, the scientific method has four steps:

* Observe and describe a natural phenomenon.
* Formulate a hypothesis to explain the phenomenon.
* Use the hypothesis to predict new observations.
* Experimentally test the predictions.

If the results of the experiment do not match the predictions, then the hypothesis is rejected and an alternative hypothesis is proposed. If the results of the experiment closely match the predictions, then belief in the hypothesis is gained, though the hypothesis will likely be subjected to further experimentation.

Statistical hypothesis testing is key to using the scientific method in many fields of study and, in fact, closely follows the scientific method in concept. Statistical hypothesis testing begins by formulating two competing statistical hypotheses from a research hypothesis. One of these hypotheses (the null) is used to predict the parameter of interest. Data is then collected and statistical methods are used to determine whether the observed statistic closely matches the prediction made from the null hypothesis or not. Probability (Module \@ref(Probability)) is used to measure the degree of matching with sampling variability taken into account. This process and the theory underlying statistical hypothesis testing is explained in detail in this module.

## Statistical Hypotheses
Hypotheses are classified into two types: (1) research hypothesis and (2) statistical hypotheses. A research hypothesis is a "wordy" statement about the question or phenomenon that the researcher is testing. Four example research hypotheses are:

* A medical researcher is concerned that a new medicine may change patients' mean pulse rate (from the "known" mean pulse rate of 82 bpm for individuals in the study population not using the new medicine).
* A chemist has invented an additive to car batteries that she thinks will extend the current 36 month average life of a battery.
* An engineer wants to determine if a new type of insulation will reduce the average heating costs of a typical house (which are currently \$145 per month).
* A researcher is concerned whether, on average, Alzheimer's caregivers at a particular facility are clinically depressed (as suggested by a mean Beck Depression Inventory (BDI) score greater than 25)

Research hypotheses are converted to statistical hypotheses that are mathematical and more easily subjected to statistical methods. There are two types of statistical hypotheses: (1) the null hypothesis and (2) the alternative hypothesis. The **null hypothesis**, abbreviated as H~0~, is a specific statement of no difference between a parameter and a specific value or between two parameters. The H~0~ ALWAYS contains an equals sign because it always represents "no difference."  The **alternative hypothesis**, abbreviated as H~A~, always states that there is some sort of difference between a parameter and a specific value or between two parameters. The type of difference comes from the research hypothesis and will require use of a less than ("<"), greater than (">"), or not equals ($\neq$) sign. Null and alternative hypotheses that correspond to the four research hypotheses above are:

* H~A~:$\mu$$\neq$82 and H~0~:$\mu$=82 (where $\mu$ represents the mean pulse rate for individuals in the study population that take the new medicine; thus, the alternative hypothesis represents a change from the "normal" pulse rate).
* H~A~:$\mu$>36 and H~0~:$\mu$=36 (where $\mu$ represents the mean life of batteries with the new additive; thus, this alternative hypothesis represents an extension of the current battery life).
* H~A~:$\mu$<145 and H~0~:$\mu$=145 (where $\mu$ represents the mean monthly heating bill for houses that receive the new type of insulation; thus, this alternative hypothesis represents a decline in heating bills from the previous "normal" amount).
* H~A~:$\mu$>25 and H~0~:$\mu$=25 (where $\mu$ represents the mean BDI score; thus, this alternative hypothesis represents a mean score that indicates clinical depression).

The sign used in the alternative hypothesis comes directly from the wording of the research hypothesis (Table \@ref(tab:HAwords)). An alternative hypothesis that contains the $\neq$ sign is called a **two-tailed alternative**, as the value can be "not equal" to another value in two ways; i.e., less than or greater than. Alternative hypotheses with the < or the > signs are called **one-tailed alternatives**. The null hypothesis is easily constructed from the alternative hypothesis by replacing the sign in the alternative hypothesis with an equals sign.

```{r HAwords, echo=FALSE}
tmp <- tibble(lt=c("is less than","is below","is lower than",
            "is shorter than","is smaller than","is reduced from",
            "is at most","is not more than"),
       gt=c("is greater than","is more than","is larger than","is longer than",
            "is bigger than","is better than","is at least","is not less than"),
       ne=c("is not equal to","is different from","has changed from",
            "is not the same as",NA,NA,NA,NA))
names(tmp) <- c("<",">","$\\neq$")
tmp %>%
  knitr::kable(booktabs=TRUE,align="c",caption="Common words that indicate which sign to use in the alternative hypothesis.") %>%
  kableExtra::kable_classic("hover",full_width=FALSE,html_font=khfont) %>%
  kableExtra::row_spec(0,bold=TRUE) %>%
  kableExtra::column_spec(1:3,width="1.6in")
```

### Hypothesis Testing Concept
Statistical hypothesis testing begins by using the null hypothesis to predict what value one should expect for the mean in a sample. So, for the Square Lake example (from Module \@ref(WhyStats)), if H~0~:$\mu$=105 and H~A~:$\mu$<105, then one would expect, if the null hypothesis is true, that the observed sample mean would be 105. If the observed sample mean was NOT equal to 105 and sampling variability did not exist, then the prediction based on the null hypothesis would not be supported and one would conclude that the null hypothesis was incorrect. In other words, one would conclude that the population mean was not equal to 105.

Of course, sampling variability does exist and it complicates matters. The simple interpretation of not supporting H~0~ because the observed sample mean did not equal the hypothesized population mean canNOT be made because, with sampling variability, one would not expect a statistic to exactly equal the parameter in the population from which the sample was extracted. For example, even if the null hypothesis was correct, one would not expect, with sampling variability, the observed sample mean to exactly equal 105; rather, one would expect the observed sample mean to be **reasonably** close to 105.

Thus, hypothesis testing is a process to determine if the difference between the observed statistic and the expected statistic based on the null hypothesis is "large" **relative to sampling variability**. For example, the standard error of $\bar{x}$ for samples of n=50 in the Square Lake example is $\frac{\sigma}{\sqrt{n}}=$$\frac{31.5}{\sqrt{50}}$=4.45. With this sampling variability, an observed sample mean of 103 would be considered reasonably close to 105 and one would have more belief in H~0~:$\mu$=105 (Figure \@ref(fig:HOSLExample)). However, an observed sample mean of 90 is further away from 105 than one would expect based on sampling variability alone and belief in H~0~:$\mu$=105 would lessen (Figure \@ref(fig:HOSLExample)).

&nbsp;

```{r HOSLExample, echo=FALSE, fig.width=5.5, fig.cap="Sampling distribution of samples means of n=50 from the Square Lake population ASSUMING that $\\mu$=105."}
par(mar=c(3,1.5,0.5,1.5),mgp=c(1.9,0.4,0),yaxs="i",tcl=-0.2,xpd=TRUE)
mu <- 105
SE <- 31.5/sqrt(50)
x0 <- seq(mu-5*SE,mu+5*SE,length.out=200)
norm0 <- dnorm(x0,mean=mu,sd=SE)
plot(x0,norm0,type="l",lwd=3,bty="n",
     xlab="Mean Total Length (mm)",
     ylab="",yaxt="n",xaxt="n")
axis(1,round(mu+(-3:3)*SE,0))
axis(1,round(mu+c(0,3)*SE,0))
abline(h=0)
arrows(103,0.01,103,0,length=0.15,col="blue2",lwd=2)
arrows(103,0.01,112,0.06,code=0,col="blue2",lwd=2)
text(118.8,0.07,"Given sampling variability;\na mean of 103 is likely\nMORE belief in Ho",col="blue2",font=2,xpd=TRUE)
arrows(90,0.06,90,0,length=0.15,col="darkred",lwd=2)
text(91,0.06,"Given sampling variability;\na mean of 90 is UNlikely\nLESS belief in Ho",col="darkred",font=2,pos=3,xpd=TRUE)
```

&nbsp;

While the above procedure is intuitively appealing, the conclusions are not as clear when the examples chosen (i.e., samples means of 103 and 90) are not as extremely close or distant from the null hypothesized value. For example, what would one concludeif the observed sample mean was 97?. A first step in creating a more objective decision criteria is to compute the "p-value." A p-value is the probability of the observed statistic or a value of the statistic more extreme assuming that the null hypothesis is true. The p-value is described in more detail below given its centrality to making conclusions about statistical hypotheses.

The meaning of the phrase "or more extreme" in the p-value definition is derived from the sign in H~A~ (Figure \@ref(fig:HOtails)). If H~A~ is the "less than" situation, then "or more extreme" means "less than" or "shade to the left" for the probability calculation. The "greater than" situation is defined similarly but would result in shading to the "right."  In the "not equals" situation, "or more extreme" means further into the tail AND the exact same size of tail on the other side of the distribution. It is clear from Figure \@ref(fig:HOtails) why "less than" and "greater than" are one-tailed alternatives and "not equals" is a two-tailed alternative.

&nbsp;

```{r HOtails, echo=FALSE, fig.width=6, fig.height=2, fig.cap='Depiction of "or more extreme" (red areas) in p-values for the three possible alternative hypotheses.'}
par(mfcol=c(1,3),mar=c(2,0,2,0),las=1,tcl=-0.2)
x0 <- seq(-4,4,by=0.001)
norm0 <- dnorm(x0,0,1)
plot(x0,norm0,type="l",xlab="",ylab="",axes=FALSE,lwd=3)
mtext(expression(bold(H[A]:mu<mu[0])),3)
cv1 <- -1.5
xc1 <- c(cv1,x0[x0<=cv1],cv1)
yc1 <- c(0,norm0[x0<=cv1],0)
polygon(xc1,yc1,col="red",border="red")
lines(x0,norm0,lwd=3)
text(cv1,-0.03,expression(bar(x)),cex=2,xpd=TRUE)

plot(x0,norm0,type="l",xlab="",ylab="",axes=FALSE,lwd=3)
mtext(expression(bold(H[A]:mu>mu[0])),3)
cv2 <- 1.5
xc2 <- c(cv2,x0[x0>=cv2],cv2)
yc2 <- c(0,norm0[x0>=cv2],0)
polygon(xc2,yc2,col="red",border="red")
lines(x0,norm0,lwd=3)
text(cv2,-0.03,expression(bar(x)),cex=2,xpd=TRUE)

plot(x0,norm0,type="l",xlab="",ylab="",axes=FALSE,lwd=3)
mtext(expression(bold(H[A]:mu!=mu[0])),3)
polygon(xc1,yc1,col="red",border="red")
polygon(xc2,yc2,col="red",border="red")
lines(x0,norm0,lwd=3)
text(cv2,-0.03,expression(bar(x)),cex=2,xpd=TRUE)
```

&nbsp;

The "assuming that the null hypothesis is true" phrase is used to define a $\mu$ for the sampling distribution on which the p-value will be calculated. This sampling distribution is called the **null distribution** because it depends on the value of $\mu$ from the null hypothesis. One must remember that the null distribution represents the distribution of all possible sample means assuming that the null hypothesis is true; it does NOT represent the actual sample means.^[Of course, unless the null hypothesis happens to be perfectly true.]  The null distribution in the Square Lake example is thus $\bar{x}\sim N(105,4.45)$ because n=50>30 (so the Central Limit Theorem holds), H~0~:$\mu$=105, and SE=$\frac{31.49}{\sqrt{50}}$=4.45.

The p-value is computed with a "forward" normal distribution calculation on the null sampling distribution. For example, suppose that a sample mean of 100 was observed with n=50 from Square Lake (as it was in Table \@ref(tab:SquareLakeSample1s)). The p-value in this case would be "the probability of observing $\bar{x}$=100 or a smaller value assuming that $\mu$=105."  This probability is computed by finding the area to the left of 100 on a N(105,4.45) null distribution and is the exact same type of calculation as that made in Section \@ref(probability-of-statistics). Thus, this p-value of `r kPvalue(pnorm(100,mean=105,sd=31.49/sqrt(50)),latex=FALSE)` is computed as below and shown in Figure \@ref(fig:SLpvalue1).

```{r SLpvalue1, echo=-1, results="hide", fig.cap="Depiction of the p-value for the Square Lake example where $\\bar{x}=100$ and $H_{A}:\\mu<105$."}
par(mar=c(3.05,3.05,3.05,2),mgp=c(1.9,0.3,0),tcl=-0.2,las=1,
    cex.lab=0.95,cex.axis=0.9)
( distrib(100,mean=105,sd=31.49/sqrt(50)) )
```

&nbsp;

Interpreting the p-value requires critically thinking about the p-value definition and how it is calculated. Small p-values appear when the observed statistic is "far" from the null hypothesized value. In this case there is a small probability of seeing the observed statistic ASSUMING that H~0~ is true. Thus, the assumption is likely wrong and H~0~ is likely incorrect. In contrast, large p-values appear when the observed statistic is close to the null hypothesized value suggesting that the assumption about H~0~ may be correct.

The p-value serves as a numerical measure on which to base a conclusion about H~0~. To do this objectively requires an objective definition of what it means to be a "small" or "large" p-value. Statisticians use a cut-off value, called the rejection criterion and symbolized with $\alpha$, such that p-values less than $\alpha$ are considered small and would result in rejecting H~0~ as a viable hypothesis. The value of $\alpha$ is typically small, usually set at 0.05, although $\alpha$=0.01 and $\alpha$=0.10 are also commonly used.

The choice of $\alpha$ is made by the person conducting the hypothesis test and is based on how much evidence a researcher demands before rejecting H~0~. Smaller values of $\alpha$ require a larger difference between the observed statistic and the null hypothesized value and, thus, require "more evidence" of a difference for the H~0~ to be rejected. For example, if rejection of the null hypothesis will be heavily scrutinized by regulatory agencies, then the researcher may want to be very sure before claiming a difference and should then set $\alpha$ at a smaller value, say $\alpha$=0.01. The actual choice for $\alpha$ MUST be made before collecting any data and canNOT be changed once the data has been collected. In other words, once the data are in hand, a researcher cannot lower or raise $\alpha$ to achieve a desired outcome regarding H~0~.

::: {.tip data-latex=''}
The value of the rejection criterion ($\alpha$) is set by the researcher BEFORE data is collected.}
:::

The null hypothesis in the Square Lake example is not rejected because the p-value (i.e., `r kPvalue(pnorm(100,mean=105,sd=31.49/sqrt(50)),include.p=FALSE,latex=FALSE)`) is larger than any of the common values of $\alpha$. Thus, the conclusion in this example is that it is possible that the mean of the entire population is equal to 105 and it is not likely that the population mean is less than 105. In other words, observing a sample mean of 100 is likely to happen based on random sampling variability alone and it is unlikely that the null hypothesized value is incorrect.


## Hypothesis Testing Concept Summary
In summary, hypotheses are statistically examined with the following procedure.

* Construct null and alternative hypotheses from the research hypothesis.
* Construct an expected value of the statistic based on the null hypothesis (i.e., assume that the null hypothesis is true).
* Calculate an observed statistic from the individuals in a sample.
* Compare the difference between the observed statistic and the expected statistic based on the null hypothesis in relation to sampling variability (i.e., calculate a test statistic and p-value).
* Use the p-value to determine if this difference is "large" or not.
    * If this difference is "large" (i.e., p-value<$\alpha$), then reject the null hypothesis.
    * If this difference is not "large" (i.e., p-value>$\alpha$), then "Do Not Reject" the null hypothesis.

Statisticians say "do not reject H~0~" rather than "accept H~0~ as true" when the p-value >$\alpha$ for two reasons. First, there are several other possible values, besides the specific value in the null hypothesis, that would lead to "do not reject" conclusions. For example, if a null hypothesized value of 105 was not rejected, then values of 104.99, 104.98, etc. would also likely not be rejected.^[In fact, for example, the values in a 95% confidence interval -- see Module \@ref(ConfRegions1) -- represent all possible hypothesized values that would not be rejected with a two-tailed H~A~ using $\alpha$=0.05.] So, we don't say that we "accept" a particular hypothesized value when we know many other values would also be "accepted."

Second, the null hypothesis is almost always not true. Consider the null hypothesis of the Square Lake example (i.e., "that the mean length is 105 mm"). The mean length of fish in Square Lake is undoubtedly not exactly equal to 105. It may be 104.9, 105.01, or some other more disparate value. The point is that the specific value of the hypothesis is likely never true, especially for a continuous variable. The problem is that it takes large amounts of data to be able to distinguish means that are very close to the true population mean (i.e., it is difficult to distinguish between 104.9 and 105 when sampling variability is present). Very often we will not take a sample size large enough to distinguish these subtle differences. Thus, we will say that we "do not reject H~0~" because there simply was not enough data to reject it.

&nbsp;
