# Hypothesis Testing - Errors {#HypTesting2}
XXX

## Errors
The goal of hypothesis testing is to make a decision about H~0~. Unfortunately, because of sampling variability, there is always a risk of making an incorrect decision. Two types of incorrect decisions can be made (Table \@ref(tab:DMerrs)). A Type I error occurs when a true H~0~ is falsely rejected. In other words, even if H~0~ is true, there is a chance that a rare sample will occur and H~0~ will be deemed incorrect. The probability of making a Type I error is set when $\alpha$ is chosen. A Type II error occurs when a false H~0~ is not rejected. The probability of a Type II error is denoted by $\beta$.

&nbsp;

```{r DMerrs, echo=FALSE}
tibble("Truth about Popn"=c("H~0~ is true","H~0~ is true",
                            "H~A~ is true","H~A~ is true"),
       "Decision from Test"=c("Reject H~0~","DNR H~0~",
                              "Reject H~0~","DNR H~0~"),
       "Error Type"=c("Type I ($\alpha$)","--","--","Type II ($\beta$)")) %>%
  knitr::kable(booktabs=TRUE,caption="Types of decisions that can be made from a hypothesis test.",align="c") %>%
  kableExtra::kable_classic("hover",full_width=FALSE,html_font=khfont) %>%
  kableExtra::row_spec(0,bold=TRUE) %>%
  kableExtra::column_spec(1:3,width="1.6in")
```

&nbsp;

The decision in the Square Lake example of Module \@ref(HypTesting1) produced a Type II error because H~0~:$\mu$=105 was not rejected even though we know that $\mu$=98.06 (Table \@ref(tab:SquareLakePopnTbl)). Unfortunately, in real life, it will never be known exactly when a Type I or a Type II error has been made because the true $\mu$ is not known. However, it is known that a Type I error will be made $100\alpha$\% of the time. The probability of a type II error ($\beta$), though, is never known because this probability depends on the true but unknown $\mu$. Decisions can be made, however, that affect the magnitude of $\beta$.

Even though $\beta$ can not usually be calculated, a researcher can make decisions that will reduce $\beta$ (Figure \@ref(fig:SLBetaRelations)). For example, a researcher can decrease $\beta$ by increasing $\alpha$ or $n$. Increasing $n$ is more beneficial because it does not result in an increase in Type I errors as would occur with increasing $\alpha$.

In addition, $\beta$ increases as the difference between the hypothesized mean ($\mu_{0}$) and the actual mean ($\mu_{A}$) decreases (Figure \@ref(fig:SLBetaRelations)). This means that you will make more Type II errors when the hypothesized and actual mean are close together. In addition, $\beta$ increases with a increasing amount of natural variability (i.e., $\sigma$; Figure \@ref(fig:SLBetaRelations)). In other words, more Type II errors are made when there is more variability among individuals. A researcher cannot control the difference between $\mu_{0}$ and $\mu_{A}$ or the value of $\sigma$. However, it is important to know that if a situation with a "large" amount of variability is encountered or the difference to be detected is small, the researcher will need to increase $n$ to reduce $\beta$. For example, if $n$ could be doubled in the Square Lake example to 100, then $\beta$ (for H~0~:$\mu=105$) would decrease to approximately 0.18 (Figure \@ref(fig:SLBetaRelations)).

&nbsp;

```{r SLBetaRelations, echo=FALSE, fig.width=6.5, fig.height=6.5, fig.cap="The relationship between one-tailed (lower) $\\beta$ and $\\alpha$, $n$, actual mean ($\\mu_{A}$), and $\\sigma$. In all situations where the variable does not vary, $\\mu_{0}=105$, $\\mu_{A}=98.06$, $\\sigma=31.49$, $n=50$, and $\\alpha=0.05$. "}
beta <- function(mu0,mua,sigma,n,alpha,uptail=FALSE){
  SE <- sigma/sqrt(n)
  if (uptail) pnorm(qnorm(1-alpha,mu0,SE),mua,SE)
  else 1-pnorm(qnorm(alpha,mu0,SE),mua,SE)
}

# Set the Square Lake values
mu0 <- 105
muA <- 98.06
sigma <- 31.49
n <- 50
alpha <- 0.05
# Cycle through alphas
alphas <- seq(0.002,0.1,by=0.002)
balpha <- beta(mu0,muA,sigma,n,alphas,FALSE)
plota <- ggplot(data.frame(alphas,balpha),mapping=aes(x=alphas,y=balpha)) +
  geom_line(size=1) +
  coord_cartesian(ylim=c(0,1)) +
  labs(x=expression(alpha),y=expression(beta)) +
  theme_NCStats() +
  annotate(geom="text",x=Inf,y=Inf,hjust=1.1,vjust=1.1,
           label="beta DEcreases with\n INcreasing alpha", color="darkblue")
# Cycle through sample sizes
ns <- seq(2,250)
bn <- beta(mu0,muA,sigma,ns,alpha,FALSE)
plotn <- ggplot(data.frame(ns,bn),mapping=aes(x=ns,y=bn)) +
  geom_line(size=1) +
  coord_cartesian(ylim=c(0,1)) +
  labs(x="n",y=expression(beta)) +
  theme_NCStats() +
  annotate(geom="text",x=Inf,y=Inf,hjust=1.1,vjust=1.1,
           label="beta DEcreases with\n INcreasing n",
           color="darkblue")
# Cycle through sigmas
sigmas <- seq(1,60,by=0.5)
bsigma <- beta(mu0,muA,sigmas,n,alpha,FALSE)
plots <- ggplot(data.frame(sigmas,bsigma),mapping=aes(x=sigmas,y=bsigma)) +
  geom_line(size=1) +
  coord_cartesian(ylim=c(0,1)) +
  labs(x=expression(sigma),y=expression(beta)) +
  theme_NCStats() +
  annotate(geom="text",x=-Inf,y=Inf,hjust=-0.1,vjust=1.1,
           label="beta INcreases with\n INcreasing sigma",
           color="darkblue")
# Cycle through muas
muas <- seq(85,105,by=1)
bmua <- beta(mu0,muas,sigma,n,alpha,FALSE)
plotm <- ggplot(data.frame(muas,bmua),mapping=aes(x=muas,y=bmua)) +
  geom_line(size=1) +
  coord_cartesian(ylim=c(0,1)) +
  labs(x=expression(mu[A]),y=expression(beta)) +
  theme_NCStats() +
  annotate(geom="text",x=-Inf,y=Inf,hjust=-0.1,vjust=1.1,
           label="beta INcreases with\n INcreasing difference\n from truth (=105)",
           color="darkblue")
plota + plotn + plots + plotm
```

&nbsp;

A concept that is very closely related to decision-making errors is the idea of statistical power, or just **power** for short. Power is the probability of correctly rejecting a false H~0~. In other words, it is the probability of detecting a difference from the hypothesized value if a difference really exists. Power is used to demonstrate how sensitive a hypothesis test is for identifying a difference. High power related to a H~0~ that is not rejected implies that the H~0~ really should not have been rejected. Conversely, low power related to a H~0~ that was not rejected implies that the test was very unlikely to detect a difference, so not rejecting H~0~ is not surprising nor particularly conclusive. Power is equal to $1-\beta$.

&nbsp;

## Test Statistics and Effect Sizes
Instead of reporting the observed statistic and the resulting p-value, it may be of interest to know how "far" the observed statistic was from the hypothesized value of the parameter. This is easily calculated with

$$ \text{Observed Statistic}-\text{Hypothesized Parameter} $$

where "Hypothesized Parameter" represents the specific value in H~0~. However, the meaning of this difference is difficult to interpret without an understanding of the standard error of the statistic. For example, a difference of 10 between the observed statistic and the hypothesized parameter seems "very different" if the standard error is 3 but does not seem "different" if the standard error is 15 (Figure \@ref(fig:EffectSizeSE)).

&nbsp;

```{r EffectSizeSE, echo=FALSE, fig.width=5.5, fig.cap="Sampling distribution of samples means with SE=3 (Left) and SE=15 (Right). A single observed sample mean of 90 (a difference of 10 from the hypothesized mean of 100) is shown by the red dot and arrow."}
par(mfrow=c(1,2),mar=c(3,1.5,1.5,1.5),mgp=c(1.9,0.4,0),yaxs="i",tcl=-0.2)
mu <- 100
xbar <- 90
SE <- c(3,15)
zext <- 3
x0 <- seq(mu-zext*SE[2],mu+zext*SE[2],length.out=200)
norm0 <- dnorm(x0,mean=mu,sd=SE[1])
norm1 <- dnorm(x0,mean=mu,sd=SE[2])
xlim <- range(x0)
plot(x0,norm0,type="l",lwd=3,bty="n",xaxt="n",yaxt="n",xlim=xlim,
     xlab="Sample Means",ylab="")
points(xbar,0.0005,col="red",pch=16,bg="red",xpd=TRUE,cex=1.25)
axis(1,c(60,80,120,140))
axis(1,100,col.ticks="blue2",col.axis="blue2",tcl=-0.3)
rect(50,0.084,150,0.116,col="white",border=NA)
text(100,0.1,"A difference of 10\nappears MORE\nextreme with SE=3",
     col="darkred",font=2)
arrows(90,0.04,90,0.001,col="red",length=0.15,angle=15)

plot(x0,norm1,type="l",lwd=3,bty="n",xaxt="n",yaxt="n",xlim=xlim,
     xlab="Sample Means",ylab="")
points(xbar,0.0005,col="red",pch=16,bg="red",xpd=TRUE,cex=1.25)
axis(1,c(60,80,120,140))
axis(1,100,col.ticks="blue2",col.axis="blue2",tcl=-0.3)
rect(50,0.017,150,0.0229,col="white",border=NA)
text(100,0.02,"A difference of 10\nappears LESS\nextreme with SE=15",
     col="blue3",font=2)
arrows(90,0.008,90,0.0007,col="red",length=0.15,angle=15)
```

&nbsp;

The difference between the observed statistic and the hypothesized parameter is  standardized to a common scale by dividing by the standard error of the statistic. The result is called a *test statistic* and is generalized with

$$ \text{Test Statistic} = \frac{\text{Observed Statistic}-\text{Hypothesized Parameter}}{SE_{\text{Statistic}}} $$

Thus, the test statistic measures how many standard errors the observed statistic is away from the hypothesized parameter. A relatively large value is indicative of a difference that is likely not due to randomness (i.e., sampling variability) and suggests that the null hypothesis should be rejected.

The test statistic in the Square Lake Example is $\frac{100-105}{\frac{31.49}{\sqrt{50}}}$=-1.12. Thus, the observed mean total length of 100 mm is 1.12 standard errors below the null hypothesized mean of 105 mm. From our experience, a little over one SE from the mean is not "extreme" and, thus, it is not surprising that the null hypothesis was not rejected.

There are other forms for calculating test statistics, but all test statistics retain the general idea of scaling the difference between what was observed and what was expected from the null hypothesis in terms of sampling variability. Even though there is a one-to-one relationship between a test statistic and a p-value, a test statistic is often reported with a hypothesis test to give another feel for the magnitude of the difference between what was observed and what was predicted.

&nbsp;
