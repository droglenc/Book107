# Hypothesis Testing - Errors {#HypTesting2}
In Module \@ref(HypTesting1) we examined the concept of hypothesis testing and showed how a p-value is calculated and used to reject or not reject H~0~. The basic concept there is that if the p-value is small (<&alpha;) then it unlikely that the observed statistic would come from a population with the parameter in the H~0~. It is important to note the word *unlikely* in this statement. It is possible that the observed statistic would come from this population, just unlikely. In the case where this unlikely event actually happened, we would make an error in our conclusion about H~0~. These errors are discussed in this module.

## Error Types
The goal of hypothesis testing is to make a decision about H~0~. Unfortunately, because of sampling variability, there is always a risk of making an incorrect decision. Two types of incorrect decisions can be made (Table \@ref(tab:DMerrs)). A **Type I error** occurs when a true H~0~ is falsely rejected. In other words, even if H~0~ is true, there is a chance that a rare sample will occur and H~0~ will be deemed incorrect. A **Type II error** occurs when a false H~0~ is not rejected.

```{r DMerrs, echo=FALSE}
tibble("Truth about Popn"=c("H~0~ is true","H~0~ is true",
                            "H~A~ is true","H~A~ is true"),
       "Decision from Test"=c("Reject H~0~","DNR H~0~",
                              "Reject H~0~","DNR H~0~"),
       "Error Type"=c("Type I ($\\alpha$)","--","--","Type II ($\\beta$)")) %>%
  knitr::kable(booktabs=TRUE,caption="Types of decisions that can be made from a hypothesis test.",align="c") %>%
  kableExtra::kable_classic("hover",full_width=FALSE,html_font=khfont) %>%
  kableExtra::row_spec(0,bold=TRUE) %>%
  kableExtra::column_spec(1:3,width="1.6in")
```

&nbsp;

It is important to be able to articulate what Type I and a Type II errors are within the context of a specific situation. For example, suppose that if the mean abundance of a rare plant in a particular area drops below 0.5 plants per m^2^ then there is a real concern that that plant will become extinct. A researcher may conduct a study to determine if the mean abundance of this plant has dropped below the 0.5 m^2^ threshold or not. The null and alternative hypotheses would be

* H~0~: &mu;=0.5, where &mu; is the mean abundance of the plant per m^2^
* H~A~: &mu;<0.5

Before articulating what the errors would be in this example, it is useful to write in words what the hypotheses are within the context of the situation.

* H~0~: "abundance at or above the threshold; population not at risk of extinction"
* H~A~: "abundance below the threshold; population at risk of extinction"

From the definition of a Type I error (i.e., incorrectly rejecting a true H~0~), a Type I error occurs if we conclude that H~A~ is true when H~0~ is really true. In this scenario then, a Type I error would be concluding that the abundance of the plant is low (*think H~A~ is true*) when the abundance is really not low (*H~0~ is really true*) or concluding that the population is at risk of extinction when the population is really not at risk of extinction.

From the definition of a Type II error (i.e., incorrectly not rejecting a false H~0~), a Type II error is concluding that H~0~ is true when H~A~ is really true. In this scenario then, a Type II error is concluding that the abundance of the plant is not low (*think H~0~ is true*) when the abundance is really low (*H~A~ is really true*) or concluding that the population is not at risk of extinction when the population is really at risk of extinction.

## Error Rates
The decision in the Square Lake example of Module \@ref(HypTesting1) was a Type II error because H~0~:&mu;=105 was not rejected even though we know that &mu;=98.06 (Table \@ref(tab:SquareLakePopnTbl)). Unfortunately, in real life, it will never be known exactly when a Type I or a Type II error has been made because the true &mu; is not known. However, the rates at which these errors are made can be considered.

The probability of making a Type I error is set when &alpha; is chosen. Thus, the researcher can largely choose the rate at which they will make a Type I error. The probability of a Type II error is denoted by &beta;, which is never known because calculating &beta; requires knowing the true but unknown &mu;. Decisions can be made, however, that affect the magnitude of &beta; (Figure \@ref(fig:SLBetaRelations)).

There are two items that affect &beta; that a researcher can control -- the size of &alpha; and n. The &beta; decreases as &alpha; increases (Figure \@ref(fig:SLBetaRelations)); i.e., the researcher is reducing Type II errors by allowing for more Type errors. In other words, the researcher is simply "trading errors," which may be appropriate if a Type II error is more egregious than a Type I error. The &beta; also decreases with increasing n (Figure \@ref(fig:SLBetaRelations)); i.e., fewer errors are made as more information is gathered. Of these two choices, reducing &beta; by increasing n is generally more beneficial because it does not result in an increase in Type I errors as would occur with increasing &alpha;.

&nbsp;

```{r SLBetaRelations, echo=FALSE, fig.width=6.5, fig.height=6.5, fig.cap="The relationship between one-tailed (lower) &beta; and &alpha;, n, actual mean (&mu;~A~), and &sigma;. In all situations where the variable does not vary, &mu;~0~=105, &mu;~A~=98.06, &sigma;=31.49, n=50, and &alpha;=0.05."}
beta <- function(mu0,mua,sigma,n,alpha,uptail=FALSE){
  SE <- sigma/sqrt(n)
  if (uptail) pnorm(qnorm(1-alpha,mu0,SE),mua,SE)
  else 1-pnorm(qnorm(alpha,mu0,SE),mua,SE)
}

# Set the Square Lake values
mu0 <- 105
muA <- 98.06
sigma <- 31.49
n <- 50
alpha <- 0.05
# Cycle through alphas
alphas <- seq(0.002,0.1,by=0.002)
balpha <- beta(mu0,muA,sigma,n,alphas,FALSE)
plota <- ggplot(data.frame(alphas,balpha),mapping=aes(x=alphas,y=balpha)) +
  geom_line(size=1) +
  coord_cartesian(ylim=c(0,1)) +
  labs(x=expression(alpha),y=expression(beta)) +
  theme_NCStats() +
  annotate(geom="text",x=Inf,y=Inf,hjust=1.1,vjust=1.1,
           label="beta DEcreases with\n INcreasing alpha", color="darkblue")
# Cycle through sample sizes
ns <- seq(2,250)
bn <- beta(mu0,muA,sigma,ns,alpha,FALSE)
plotn <- ggplot(data.frame(ns,bn),mapping=aes(x=ns,y=bn)) +
  geom_line(size=1) +
  coord_cartesian(ylim=c(0,1)) +
  labs(x="n",y=expression(beta)) +
  theme_NCStats() +
  annotate(geom="text",x=Inf,y=Inf,hjust=1.1,vjust=1.1,
           label="beta DEcreases with\n INcreasing n",
           color="darkblue")
# Cycle through sigmas
sigmas <- seq(1,60,by=0.5)
bsigma <- beta(mu0,muA,sigmas,n,alpha,FALSE)
plots <- ggplot(data.frame(sigmas,bsigma),mapping=aes(x=sigmas,y=bsigma)) +
  geom_line(size=1) +
  coord_cartesian(ylim=c(0,1)) +
  labs(x=expression(sigma),y=expression(beta)) +
  theme_NCStats() +
  annotate(geom="text",x=-Inf,y=Inf,hjust=-0.1,vjust=1.1,
           label="beta INcreases with\n INcreasing sigma",
           color="darkblue")
# Cycle through muas
muas <- seq(85,105,by=1)
bmua <- beta(mu0,muas,sigma,n,alpha,FALSE)
plotm <- ggplot(data.frame(muas,bmua),mapping=aes(x=muas,y=bmua)) +
  geom_line(size=1) +
  coord_cartesian(ylim=c(0,1)) +
  labs(x=expression(mu[A]),y=expression(beta)) +
  theme_NCStats() +
  annotate(geom="text",x=-Inf,y=Inf,hjust=-0.1,vjust=1.1,
           label="beta INcreases with\n INcreasing difference\n from truth (=105)",
           color="darkblue")
plota + plotn + plots + plotm
```

&nbsp;

The value of &beta; is als related to two items that a researcher cannot control. The &beta; increases as the difference between the hypothesized mean (&mu;~0~) and the actual mean (&mu;~A~) decreases (Figure \@ref(fig:SLBetaRelations)). This means that more Type II errors will be made when the hypothesized and actual mean are close together. In other words, more Type II errors are made when it is harder to distinguish the hypothesized mean from the actual mean.

In addition, &beta; increases with increasing amounts of natural variability (i.e., &sigma;; Figure \@ref(fig:SLBetaRelations)). In other words, more Type II errors are made when there is more variability among individuals.

A researcher cannot control the difference between &mu;~0~ and &mu;~A~ or the value of &sigma;. However, it is important to know that if a situation with a "large" amount of variability is encountered or the difference to be detected is small, the researcher will need to increase n to reduce &beta;. For example, if n could be doubled in the Square Lake example to 100, then &beta; (for H~0~:&mu;=105) would decrease to approximately 0.18 (Figure \@ref(fig:SLBetaRelations)).

#### Statistical Power {-}
A concept that is very closely related to decision-making errors is the idea of statistical power, or just **power** for short. Power is the probability of correctly rejecting a false H~0~. In other words, it is the probability of detecting a difference from the hypothesized value if a difference really exists. Power is used to demonstrate how sensitive a hypothesis test is for identifying a difference. High power related to a H~0~ that is not rejected implies that the H~0~ really should not have been rejected. Conversely, low power related to a H~0~ that was not rejected implies that the test was very unlikely to detect a difference, so not rejecting H~0~ is not surprising nor particularly conclusive. Power is equal to 1-&beta;.

&nbsp;

## Test Statistics and Effect Sizes
Instead of reporting the observed statistic and the resulting p-value, it may be of interest to know how "far" the observed statistic was from the hypothesized value of the parameter. This is easily calculated with

$$ \text{Observed Statistic}-\text{Hypothesized Parameter} $$

where "Hypothesized Parameter" represents the specific value in H~0~. However, the meaning of this difference is difficult to interpret without an understanding of the standard error of the statistic. For example, a difference of 10 between the observed statistic and the hypothesized parameter seems "very different" if the standard error is 3 but does not seem "different" if the standard error is 15 (Figure \@ref(fig:EffectSizeSE)).

&nbsp;

```{r EffectSizeSE, echo=FALSE, fig.width=5.5, fig.cap="Sampling distribution of samples means with SE=3 (Left) and SE=15 (Right). A single observed sample mean of 90 (a difference of 10 from the hypothesized mean of 100) is shown by the red dot and arrow."}
par(mfrow=c(1,2),mar=c(3,1.5,1.5,1.5),mgp=c(1.9,0.4,0),yaxs="i",tcl=-0.2)
mu <- 100
xbar <- 90
SE <- c(3,15)
zext <- 3
x0 <- seq(mu-zext*SE[2],mu+zext*SE[2],length.out=200)
norm0 <- dnorm(x0,mean=mu,sd=SE[1])
norm1 <- dnorm(x0,mean=mu,sd=SE[2])
xlim <- range(x0)
plot(x0,norm0,type="l",lwd=3,bty="n",xaxt="n",yaxt="n",xlim=xlim,
     xlab="Sample Means",ylab="")
points(xbar,0.0005,col="red",pch=16,bg="red",xpd=TRUE,cex=1.25)
axis(1,c(60,80,120,140))
axis(1,100,col.ticks="blue2",col.axis="blue2",tcl=-0.3)
rect(50,0.084,150,0.116,col="white",border=NA)
text(100,0.1,"A difference of 10\nappears MORE\nextreme with SE=3",
     col="darkred",font=2)
arrows(90,0.04,90,0.001,col="red",length=0.15,angle=15)

plot(x0,norm1,type="l",lwd=3,bty="n",xaxt="n",yaxt="n",xlim=xlim,
     xlab="Sample Means",ylab="")
points(xbar,0.0005,col="red",pch=16,bg="red",xpd=TRUE,cex=1.25)
axis(1,c(60,80,120,140))
axis(1,100,col.ticks="blue2",col.axis="blue2",tcl=-0.3)
rect(50,0.017,150,0.0229,col="white",border=NA)
text(100,0.02,"A difference of 10\nappears LESS\nextreme with SE=15",
     col="blue3",font=2)
arrows(90,0.008,90,0.0007,col="red",length=0.15,angle=15)
```

&nbsp;

The difference between the observed statistic and the hypothesized parameter is  standardized to a common scale by dividing by the standard error of the statistic. The result is called a *test statistic* and is generalized with

$$ \text{Test Statistic} = \frac{\text{Observed Statistic}-\text{Hypothesized Parameter}}{\text{SE}_{\text{Statistic}}} $$

Thus, the test statistic measures how many standard errors the observed statistic is away from the hypothesized parameter.^[This is very closely related to the concept of Z-scores and standardization introduced in Section \@ref(standardization-and-z-scores).] A relatively large value of the test statistic is indicative of a difference that is likely not due to randomness (i.e., sampling variability) and suggests that the null hypothesis should be rejected.

The test statistic in the Square Lake Example is $\frac{100-105}{\frac{31.49}{\sqrt{50}}}$=-1.12. Thus, the observed mean total length of 100 mm is 1.12 standard errors below the null hypothesized mean of 105 mm. From our experience, a little over one SE from the mean is not "extreme" and, thus, it is not surprising that the null hypothesis was not rejected.

There are other forms for calculating test statistics, but all test statistics retain the general idea of scaling the difference between what was observed and what was expected from the null hypothesis in terms of sampling variability. Even though there is a one-to-one relationship between a test statistic and a p-value, a test statistic is often reported with a hypothesis test to give another feel for the magnitude of the difference between what was observed and what was predicted.

&nbsp;
